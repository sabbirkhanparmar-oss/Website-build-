WorldPowerAI — Full Stack Starter (Preview)

This single-file project preview contains a complete starter for a full-stack "WorldPowerAI" system: frontend (React + Tailwind), backend (Node/Express API), model microservice (Python + FastAPI + Hugging Face Transformers stub), Dockerfiles, docker-compose, and a README with run/deploy instructions.


---

Project structure (logical)

worldpowerai/
├─ frontend/
│  ├─ package.json
│  ├─ tailwind.config.js
│  ├─ postcss.config.js
│  └─ src/
│     ├─ main.jsx
│     ├─ App.jsx
│     ├─ components/
│     │  ├─ ChatWindow.jsx
│     │  └─ CommandPanel.jsx
│     └─ index.css
├─ backend/
│  ├─ package.json
│  └─ src/
│     └─ index.js
├─ model_service/
│  ├─ requirements.txt
│  └─ app.py
├─ docker-compose.yml
├─ README.md
└─ .env.example


---

README.md (quick start)

# WorldPowerAI — Full Stack Starter

This repo is a starting point for an advanced assistant. It intentionally uses a small local model stub so you can test the full stack locally before scaling to heavy GPU instances.

## Prereqs
- Docker & docker-compose (recommended)
- Node 18+ / npm or yarn
- Python 3.10+

## Local quick run (with Docker)
1. Copy `.env.example` to `.env` and edit if necessary.
2. Build and run: `docker-compose up --build`
3. Open `http://localhost:3000`.

## Local run without Docker
1. Start the model service:
   ```bash
   cd model_service
   python -m venv venv && source venv/bin/activate
   pip install -r requirements.txt
   uvicorn app:app --host 0.0.0.0 --port 8000

2. Start the backend API:

cd backend
npm install
npm run dev


3. Start the frontend:

cd frontend
npm install
npm run dev



Notes

The model service is a simple FastAPI wrapper around a small causal model. Replace with a production model (e.g., hosted LLM, HF Inference API, or private cluster) when scaling.

Secure the API and add auth (JWT/OAuth) before exposing to the public internet.





---

.env.example

FRONTEND_URL=http://localhost:3000
BACKEND_URL=http://localhost:8080
MODEL_URL=http://model:8000
JWT_SECRET=change_this_secret
PORT=8080


---

docker-compose.yml

version: '3.8'
services:
  frontend:
    build: ./frontend
    ports:
      - '3000:3000'
    depends_on:
      - backend
    environment:
      - VITE_BACKEND_URL=http://localhost:8080
  backend:
    build: ./backend
    ports:
      - '8080:8080'
    depends_on:
      - model
    environment:
      - MODEL_URL=http://model:8000
      - JWT_SECRET=${JWT_SECRET}
  model:
    build: ./model_service
    ports:
      - '8000:8000'


---

Frontend — package.json (sketch)

{
  "name": "worldpowerai-frontend",
  "version": "0.1.0",
  "private": true,
  "scripts": {
    "dev": "vite",
    "build": "vite build",
    "preview": "vite preview"
  },
  "dependencies": {
    "react": "^18.2.0",
    "react-dom": "^18.2.0",
    "axios": "^1.4.0"
  },
  "devDependencies": {
    "vite": "^5.0.0",
    "tailwindcss": "^4.0.0",
    "postcss": "^8.0.0",
    "autoprefixer": "^10.0.0"
  }
}


---

Frontend — src/main.jsx

import React from 'react'
import { createRoot } from 'react-dom/client'
import App from './App'
import './index.css'

createRoot(document.getElementById('root')).render(<App />)


---

Frontend — src/App.jsx (default export React component)

import React, { useState, useEffect } from 'react'
import axios from 'axios'

export default function App(){
  const [messages, setMessages] = useState([])
  const [input, setInput] = useState('')
  const backend = import.meta.env.VITE_BACKEND_URL || 'http://localhost:8080'

  async function send(){
    if(!input.trim()) return
    const userMsg = {role:'user', text: input}
    setMessages(prev=>[...prev, userMsg])
    setInput('')
    try{
      const res = await axios.post(`${backend}/api/v1/chat`, {message: input})
      setMessages(prev=>[...prev, {role:'assistant', text: res.data.reply}])
    }catch(err){
      setMessages(prev=>[...prev, {role:'assistant', text: 'Error: could not reach backend.'}])
    }
  }

  return (
    <div className="min-h-screen bg-gray-50 p-6">
      <div className="max-w-3xl mx-auto bg-white rounded-2xl shadow p-6">
        <h1 className="text-2xl font-bold mb-4">WorldPowerAI — Console</h1>
        <div className="h-80 overflow-y-auto border rounded p-3 mb-4 bg-gray-100">
          {messages.map((m,i)=>(
            <div key={i} className={`mb-3 ${m.role==='user'?'text-right':''}`}>
              <div className={`inline-block px-3 py-2 rounded-lg ${m.role==='user'? 'bg-blue-500 text-white':'bg-white text-gray-800 border'}`}>
                {m.text}
              </div>
            </div>
          ))}
        </div>
        <div className="flex gap-2">
          <input value={input} onChange={e=>setInput(e.target.value)} placeholder="Ask the AI..." className="flex-1 p-3 border rounded" />
          <button onClick={send} className="px-4 py-2 rounded bg-indigo-600 text-white">Send</button>
        </div>
      </div>
    </div>
  )
}


---

Frontend — src/index.css (Tailwind minimal)

@tailwind base;
@tailwind components;
@tailwind utilities;

html,body,#root{height:100%}


---

Backend — package.json

{
  "name": "worldpowerai-backend",
  "version": "0.1.0",
  "main": "src/index.js",
  "scripts": {
    "start": "node src/index.js",
    "dev": "nodemon src/index.js"
  },
  "dependencies": {
    "express": "^4.18.2",
    "axios": "^1.4.0",
    "cors": "^2.8.5",
    "dotenv": "^16.0.0",
    "jsonwebtoken": "^9.0.0"
  }
}


---

Backend — src/index.js

import express from 'express'
import cors from 'cors'
import dotenv from 'dotenv'
import axios from 'axios'

dotenv.config()
const app = express()
app.use(cors())
app.use(express.json())

const MODEL_URL = process.env.MODEL_URL || 'http://localhost:8000'

app.post('/api/v1/chat', async (req, res)=>{
  const { message } = req.body
  if(!message) return res.status(400).json({error:'no message'})
  try{
    // send to model service
    const r = await axios.post(`${MODEL_URL}/generate`, {prompt: message})
    return res.json({reply: r.data.text})
  }catch(err){
    console.error(err?.message || err)
    return res.status(500).json({error:'model error'})
  }
})

app.get('/api/health', (req,res)=>res.json({ok:true}))

const port = process.env.PORT || 8080
app.listen(port, ()=>console.log('backend listening on', port))


---

Model service — requirements.txt

fastapi
uvicorn
pydantic
transformers
torch
sentencepiece


---

Model service — app.py (FastAPI small local model stub)

from fastapi import FastAPI
from pydantic import BaseModel
from typing import Optional

app = FastAPI()

class GenReq(BaseModel):
    prompt: str
    max_new_tokens: Optional[int] = 150

# NOTE: This example uses a tiny offline model for testing. Replace with your production model or API.
from transformers import AutoModelForCausalLM, AutoTokenizer
model_name = 'gpt2'

print('loading model', model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

@app.post('/generate')
async def generate(req: GenReq):
    prompt = req.prompt
    inputs = tokenizer(prompt, return_tensors='pt')
    outputs = model.generate(**inputs, max_new_tokens=req.max_new_tokens, do_sample=True, top_p=0.95, top_k=50)
    text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return {'text': text}

@app.get('/')
async def root():
    return {'ok': True}


---

Dockerfiles (sketch)

frontend/Dockerfile

FROM node:18-alpine
WORKDIR /app
COPY package*.json ./
RUN npm ci
COPY . .
RUN npm run build
EXPOSE 3000
CMD ["npm","run","preview"]

backend/Dockerfile

FROM node:18-alpine
WORKDIR /app
COPY package*.json ./
RUN npm ci
COPY . .
EXPOSE 8080
CMD ["node","src/index.js"]

model_service/Dockerfile

FROM python:3.10-slim
WORKDIR /app
COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
EXPOSE 8000
CMD ["uvicorn","app:app","--host","0.0.0.0","--port","8000"]


---

Security & Next Steps (high level)

1. Replace the toy local model with a production model or API (e.g., Hugging Face Inference, OpenAI, private cluster).


2. Add authentication (JWT or OAuth) to backend routes.


3. Add rate-limiting and request validation.


4. Move heavy model inference to GPU-enabled machines and use batching.


5. Add a vector database (Pinecone/Milvus) for long-term memory & retrieval-augmented generation (RAG).


6. Implement monitoring, logging, and alerts (Prometheus, Grafana).


7. If you intend to run at scale, consider Kubernetes + autoscaling, model sharding, and cost controls.




---

Final note

This preview is intentionally compact so you can iterate quickly. Copy the files into a repo (or I can output them all into separate files if you want). Good next steps: 1) Do you want me to generate the exact files (one-by-one) so you can copy/paste? 2) Or should I create a zip bundle here with runnable code?

